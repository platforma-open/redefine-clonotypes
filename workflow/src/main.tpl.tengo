// "Redefine clonotypes workflow"
wf := import("@platforma-sdk/workflow-tengo:workflow")
ll := import("@platforma-sdk/workflow-tengo:ll")
pt := import("@platforma-sdk/workflow-tengo:pt")
pframes := import("@platforma-sdk/workflow-tengo:pframes")
pSpec := import("@platforma-sdk/workflow-tengo:pframes.spec")
text := import("text")
slices := import("@platforma-sdk/workflow-tengo:slices")
maps := import("@platforma-sdk/workflow-tengo:maps")
xsv := import("@platforma-sdk/workflow-tengo:pframes.xsv")
json := import("json")

wf.prepare(func(args) {
	bundleBuilder := wf.createPBundleBuilder()
	bundleBuilder.addAnchor("main", args.anchorRef)

	// Clonotype definition columns
	for i, colId in args.clonotypeDefinition {
		bundleBuilder.addSingle(colId, "clonotypeDefinition_" + i)
	}

	// All non-normalized abundance columns
	bundleBuilder.addMulti({
		axes: [{ anchor: "main", idx: 0 }, { anchor: "main", idx: 1 }],
		annotations: {
			"pl7.app/isAbundance": "true",
			"pl7.app/abundance/normalized": "false"
		}
	}, "abundances")

	return {
		columns: bundleBuilder.build()
	}
})

wf.body(func(args) {
	columns := args.columns
	blockId := wf.blockId().getDataAsJson()

	abundanceCols := columns.getColumns("abundances")
	
	clonotypeDefinitionCols := []
	for i, _ in args.clonotypeDefinition {
		clonotypeDefinitionCols = append(clonotypeDefinitionCols, columns.getColumn("clonotypeDefinition_" + i))
	}

	ll.assert(len(abundanceCols) > 0, "No abundance columns found in the dataset")

	// Find main abundance column
	mainAbundanceColIndex := -1
	for i, c in abundanceCols {
		if c.spec.annotations["pl7.app/abundance/isPrimary"] == "true" {
			mainAbundanceColIndex = i
			break
		}
	}
	ll.assert(mainAbundanceColIndex != -1, "No primary abundance column found")
	mainAbundanceCol := abundanceCols[mainAbundanceColIndex]

	// Get axis specs
	sampleIdAxisSpec := mainAbundanceCol.spec.axesSpec[0]
	clonotypeKeyAxisSpec := mainAbundanceCol.spec.axesSpec[1]

	// Build the initial TSV for the pt workflow
	initialTsvBuilder := pframes.tsvFileBuilder()
	initialTsvBuilder.setAxisHeader(sampleIdAxisSpec, "sampleId")
	initialTsvBuilder.setAxisHeader(clonotypeKeyAxisSpec, "clonotypeKey")

	mainAbundanceHeader := "abundance_" + mainAbundanceColIndex
	
	abundanceHeaders := {}
	for i, col in abundanceCols {
		header := "abundance_" + i
		abundanceHeaders[header] = col
		initialTsvBuilder.add(col, { header: header })
	}
	
	definitionHeaders := []
	for i, col in clonotypeDefinitionCols {
		header := "clonotypeDefinition_" + i
		definitionHeaders = append(definitionHeaders, header)
		initialTsvBuilder.add(col, { header: header })
	}

	initialTsv := initialTsvBuilder.build()
	
	// --- Define the entire data pipeline in a single pt workflow ---
	ptWf := pt.workflow()
	df := ptWf.frame(initialTsv, {xsvType: "tsv"})

	// 1. Calculate per-sample totals for each abundance column for normalization
	sampleTotalAggs := []
	totalColumnHeaders := []
	for header, _ in abundanceHeaders {
		totalHeader := header + "_total"
		sampleTotalAggs = append(sampleTotalAggs, pt.col(header).sum().alias(totalHeader))
		totalColumnHeaders = append(totalColumnHeaders, totalHeader)
	}
	sampleTotalsDf := df.groupBy("sampleId").agg(sampleTotalAggs...)

	// 2. Create the new clonotype definition key
	dfWithNewKey := df.withColumns(
		pt.concatStr(slices.map(definitionHeaders, func(h) { return pt.col(h) }), {delimiter: "-"}).alias("__redefined_clonotype_key__")
	)
	
	// Calculate stats
	statsDf := dfWithNewKey.select(
		pt.col("clonotypeKey").nUnique().alias("nClonotypesBefore"),
		pt.col("__redefined_clonotype_key__").nUnique().alias("nClonotypesAfter")
	)
	statsDf.saveContent("stats.tsv")

	// 3. Aggregate by the new key and sampleId
	aggExpressions := [
		pt.col("clonotypeKey").maxBy(pt.col(mainAbundanceHeader)).alias("clonotypeKey")
	]
	for header, _ in abundanceHeaders {
		aggExpressions = append(aggExpressions, pt.col(header).sum().alias(header))
	}

	redefinedDf := dfWithNewKey.groupBy("sampleId", "__redefined_clonotype_key__").agg(aggExpressions...)

	// 4. Join with sample totals to prepare for normalization
	redefinedWithTotalsDf := redefinedDf.join(sampleTotalsDf, {on: "sampleId", how: "left"})

	// 5. Calculate normalized abundances
	normExpressions := []
	for header, _ in abundanceHeaders {
		normExpressions = append(normExpressions, 
			pt.col(header).truediv(pt.col(header + "_total")).alias("normalized_" + header)
		)
	}
	finalDfWithNorm := redefinedWithTotalsDf.withColumns(normExpressions...)
	
	// 6. Clean up intermediate total columns and save the final result
	finalDf := finalDfWithNorm.withoutColumns(totalColumnHeaders...)
	finalDf.save("final.tsv")

	// --- Run the workflow and process the output ---
	ptResult := ptWf.run()
	finalTsv := ptResult.getFile("final.tsv")
	statsTsvContent := ptResult.getFileContent("stats.tsv")
	
	// --- Construct specs for the new p-frame ---
	datasetSpec := columns.getSpec(args.anchorRef)
	trace := pSpec.makeTrace(datasetSpec, {
		type: "milaboratories.redefine-clonotypes",
		importance: 30,
		label: "Redefined Clonotypes"
	})

	newClonotypeKeySpec := maps.clone(clonotypeKeyAxisSpec)
	newClonotypeKeySpec.domain = maps.merge(newClonotypeKeySpec.domain, {"pl7.app/redefined-by": blockId})

	newAxes := [sampleIdAxisSpec, newClonotypeKeySpec]
	
	newColumns := []

	for header, col in abundanceHeaders {
		newSpecUnnormalized := maps.clone(col.spec)
		if col.id == mainAbundanceCol.id {
			newSpecUnnormalized.annotations = maps.merge(newSpecUnnormalized.annotations, {"pl7.app/isAnchor": "true"})
		}
		newColumns = append(newColumns, { column: header, spec: newSpecUnnormalized })

		newSpecNormalized := maps.clone(col.spec)
		newSpecNormalized.annotations = maps.merge(newSpecNormalized.annotations, {
			"pl7.app/abundance/normalized": "true"
		})
		if (col.spec.annotations["pl7.app/isAnchor"] == "true") {
			delete(newSpecNormalized.annotations, "pl7.app/isAnchor")
		}
		newSpecNormalized.valueType = "Double"
		newColumns = append(newColumns, { column: "normalized_" + header, spec: newSpecNormalized })
	}
	
	// Define the full spec for the import
	finalSpec := {
		axes: [{
			column: "sampleId",
			spec: sampleIdAxisSpec
		}, {
			column: "clonotypeKey",
			spec: newClonotypeKeySpec
		}],
		columns: newColumns,
		partitionKeyLength: 1
	}

	importedPf := xsv.importFile(finalTsv, "tsv", finalSpec, { splitDataAndSpec: true })

	pfBuilder := pframes.pFrameBuilder()
	for k, v in importedPf {
		pfBuilder.add(k, trace.inject(v.spec), v.data)
	}
	finalPf := pfBuilder.build()

	return {
		outputs: {
			statsTsvContent: statsTsvContent
		},
		exports: {
			pf: finalPf
		}
	}
})

